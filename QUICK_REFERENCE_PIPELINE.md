# âš¡ Quick Reference: Data-to-LLM Pipeline

## ğŸš€ 30-Second Start

```powershell
cd "c:\Users\aryan.gupta\OneDrive - Real Time Data Services Pvt Ltd\Desktop\AceBuddy-RAG"
.\run_complete_pipeline.ps1
```

## ğŸ“‹ What Gets Done

| Step | What | Duration |
|------|------|----------|
| 1ï¸âƒ£ **Clean Data** | Remove PII, duplicates, score quality | ~3-5s |
| 2ï¸âƒ£ **Ingest** | Store vectors in Chroma | ~5-10s |
| 3ï¸âƒ£ **Test** | Query with LLM (optional) | ~20-30s |

## ğŸ“‚ Output Files Created

After running, you'll have:

```
data/prepared/
â”œâ”€â”€ documents_cleaned.json      â† All cleaned documents
â”œâ”€â”€ chunks_for_rag.json         â† 100+ chunks ready for search
â””â”€â”€ preparation_report.json     â† Quality metrics
```

## ğŸ¯ Key Files for This Session

| File | Purpose | Size |
|------|---------|------|
| `scripts/data_preparation.py` | Clean & validate data | 500+ lines |
| `scripts/rag_ingestion.py` | Index into Chroma | 300+ lines |
| `scripts/full_pipeline.py` | Orchestrate everything | 400+ lines |
| `run_complete_pipeline.ps1` | Run from PowerShell | Simple |
| `RUN_PIPELINE_GUIDE.md` | Full execution guide | Detailed |

## âœ… Quality Features

- âœ… **8 PII patterns detected** (emails, phones, SSN, credit cards, IPs, DOB, passwords, API keys)
- âœ… **SHA256 duplicate detection** (prevents indexing same content twice)
- âœ… **Quality scoring** (0-1 scale, filters low quality)
- âœ… **Semantic chunking** (500 char optimal size)
- âœ… **Metadata preservation** (source, quality score, chunk index)

## ğŸ” Verify It Works

### Check data was prepared:
```powershell
ls data/prepared/
Get-Content data/prepared/preparation_report.json | ConvertFrom-Json
```

### Check API is running:
```powershell
curl http://localhost:8000/health
```

### Test a query:
```powershell
curl -X POST http://localhost:8000/chat `
  -H "Content-Type: application/json" `
  -d '{"query":"password reset","user_id":"test"}'
```

## ğŸ› ï¸ Common Commands

| Need | Command |
|------|---------|
| Run pipeline | `.\run_complete_pipeline.ps1` |
| Check services | `docker-compose ps` |
| View logs | `docker-compose logs -f acebuddy-api` |
| Just prepare data | `python scripts/data_preparation.py` |
| Just ingest | `python scripts/rag_ingestion.py` |
| Stop services | `docker-compose down` |

## ğŸš¨ Troubleshooting

| Problem | Solution |
|---------|----------|
| "Connection refused" | `docker-compose up -d` |
| "Python not found" | Install Python 3.10+ |
| "Module not found" | `pip install chromadb sentence-transformers requests` |
| "No chunks created" | Check `data/kb/` has files |

## ğŸ“Š Expected Results

After running pipeline:

```
âœ… Total documents: 9 (from data/kb/)
âœ… Chunks created: 109
âœ… Chunks ingested: 105
âœ… Quality filtered: 4
âœ… Vectors in Chroma: 105
âœ… PII patterns found: 3-5 (all redacted)
âœ… Duplicates removed: 0-2
âœ… Processing time: 15-45 seconds total
```

## ğŸ“– Where to Learn More

1. **Quick execution** â†’ `RUN_PIPELINE_GUIDE.md`
2. **Full architecture** â†’ `RAG_NLP_ANALYSIS.md`
3. **What's missing** â†’ `QUICK_STATUS_LLM_NLP.md`
4. **Implementation steps** â†’ `READY_TO_CODE_SOLUTIONS.md`
5. **API endpoints** â†’ `README.md`

## ğŸ¯ Next After Pipeline Succeeds

1. âœ… Review quality report (`preparation_report.json`)
2. âœ… Test with your own questions
3. âœ… Check response quality
4. â³ Implement advanced NLP (intent classification, response grading)
5. â³ Add more KB documents and re-run

## ğŸ’¡ Pro Tips

- Run without API test: `.\run_complete_pipeline.ps1 -SkipApiTest`
- Specify custom base dir: `.\run_complete_pipeline.ps1 -BaseDir "C:\path"`
- Check individual steps: See `full_pipeline.py` code
- Customize chunk size: Edit `data_preparation.py` line ~180
- Change quality threshold: Edit `rag_ingestion.py` line ~45

---

**You're Ready!** Just run: `.\run_complete_pipeline.ps1`
